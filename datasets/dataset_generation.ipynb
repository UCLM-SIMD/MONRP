{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incorrect-comedy",
   "metadata": {},
   "source": [
    "# Dataset generation\n",
    "Two types of projects generated into json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "awful-flight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 42,
=======
   "execution_count": 1,
>>>>>>> d98580f6 (solved issue in mimic when sampling individuals)
=======
   "execution_count": 42,
>>>>>>> a7235ed3 (solved comments from pull request, added minor local changes in some files)
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
<<<<<<< HEAD
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from typing import List\n",
    "import sys  \n",
    "sys.path.insert(0, '../datasets')\n",
    "from Dataset import Dataset\n",
    "\n",
=======
   "execution_count": 4,
=======
>>>>>>> d98580f6 (solved issue in mimic when sampling individuals)
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from typing import List\n",
<<<<<<< HEAD
    "from datasets.Dataset import Dataset\n",
>>>>>>> ecb85730 (now pbil and geneticnds keep nds from initial population, then pareto is now wider)
=======
    "import sys  \n",
    "sys.path.insert(0, '../datasets')\n",
    "from Dataset import Dataset\n",
    "\n",
>>>>>>> d98580f6 (solved issue in mimic when sampling individuals)
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "avg_len_dependencies: avg length of the list of of PBIs implied by a dependency p-->[PBIs]\n",
    "\"\"\"\n",
    "def random_dataset_generator(num_pbis: int = 20, num_stakeholders: int = 5, percentage_dependencies: float = 0.45,\n",
    "                             range_pbi_costs: List[int] = None, total_pbi_costs: int = None, range_stakeholder_importances: List[int] = None,\n",
    "                             range_stakeholder_pbis_priorities: List[int] = None, name: str = \"random\",  avg_len_dependencies: int = None) -> Dataset:\n",
    "\n",
    "    # default values for lists\n",
    "    default_range_pbi_costs = [1, 1, 2, 3, 5, 8, 13, 21, 34]  # Fibonacci\n",
    "    # 1=no importance; 5=highest importance\n",
    "    default_range_stakeholder_importances = [1, 2, 3, 4, 5]\n",
    "    default_range_stakeholder_pbis_priorities = [1, 2, 3, 4, 5]\n",
    "\n",
    "    # override values:\n",
    "    range_pbi_costs = default_range_pbi_costs if range_pbi_costs is None else range_pbi_costs\n",
    "    range_stakeholder_importances = default_range_stakeholder_importances if range_stakeholder_importances is None else range_stakeholder_importances\n",
    "    range_stakeholder_pbis_priorities = default_range_stakeholder_pbis_priorities if range_stakeholder_pbis_priorities is None else range_stakeholder_pbis_priorities\n",
    "\n",
    "    if num_pbis <= 0 or num_stakeholders <= 0 or percentage_dependencies < 0:\n",
    "        raise Exception(\n",
    "            \"Parameters num_pbis, num_stakeholders, num_dependencies must be positive integers\")\n",
    "    if total_pbi_costs is not None and total_pbi_costs <= 0:\n",
    "        raise Exception(\n",
    "            \"Total pbi cost must be positive\")\n",
    "\n",
    "    output_file: str = f\"{name}.json\"\n",
    "\n",
    "    # if given the total sum cost of all pbis->generate randomly the costs using aux function\n",
    "    if total_pbi_costs is not None:\n",
    "        pbi_costs = _constrained_sum_sample_pos(num_pbis, total_pbi_costs)\n",
    "    else:  # generate random pbi costs array\n",
    "        pbi_costs = np.random.choice(range_pbi_costs, size=num_pbis)\n",
    "\n",
    "    # generate random stakeholder importances array\n",
    "    stakeholder_importances = np.random.choice(\n",
    "        range_stakeholder_importances, size=num_stakeholders)\n",
    "\n",
    "    # generate random array of priorities for all pbis for each stakeholder\n",
    "\n",
    "\n",
    "    stakeholder_pbis_priorities = []\n",
    "    for _ in range(num_stakeholders):\n",
    "        priorities = np.random.choice(\n",
    "            range_stakeholder_pbis_priorities, size=num_pbis)\n",
    "        stakeholder_pbis_priorities.append(priorities.tolist())\n",
    "\n",
    "    # calculate amount of dependencies given the num of pbis\n",
    "    num_dependencies = math.floor(num_pbis*percentage_dependencies)\n",
    "\n",
    "    # generate valid dependencies until max and store them in a dict\n",
    "    done_dependencies = {}\n",
    "    counter_dependencies = 0\n",
    "    while(counter_dependencies < num_dependencies):\n",
    "        random_pbi1 = np.random.randint(0, num_pbis)\n",
    "\n",
    "        if avg_len_dependencies is None:\n",
    "            random_pbi2 = np.random.randint(0, num_pbis)\n",
    "            if (random_pbi1 == random_pbi2):\n",
    "                continue\n",
    "            if(random_pbi1 in done_dependencies and random_pbi2 in done_dependencies[random_pbi1]):\n",
    "                continue\n",
    "        else:\n",
    "            random_list_pbi2 = random.sample(range(num_pbis), np.random.randint(avg_len_dependencies-1, avg_len_dependencies+1))\n",
    "            if random_pbi1 in done_dependencies:\n",
    "                continue\n",
    "            if random_pbi1 in random_list_pbi2:\n",
    "                random_list_pbi2.remove(random_pbi1)\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "            done_dependencies.setdefault(random_pbi1, []).extend(random_list_pbi2)\n",
=======
    "            done_dependencies.setdefault(random_pbi1, []).append(random_list_pbi2)\n",
>>>>>>> ecb85730 (now pbil and geneticnds keep nds from initial population, then pareto is now wider)
=======
    "            done_dependencies.setdefault(random_pbi1, []).extend(random_list_pbi2)\n",
>>>>>>> d98580f6 (solved issue in mimic when sampling individuals)
    "\n",
    "        counter_dependencies += 1\n",
    "\n",
    "    # transform dict values to array of None or nested array elements\n",
    "    pbi_dependencies = np.empty(num_pbis, dtype=object)\n",
    "    for key, value in done_dependencies.items():\n",
    "        pbi_dependencies[key] = []\n",
    "        pbi_dependencies[key] = np.append(pbi_dependencies[key], value)\n",
    "        pbi_dependencies[key] = pbi_dependencies[key].tolist()\n",
    "        for x in range(len(pbi_dependencies[key])):\n",
    "            pbi_dependencies[key][x] = int(pbi_dependencies[key][x])\n",
    "\n",
    "    # return format\n",
    "    json_data = {\n",
    "        \"pbis_cost\": pbi_costs.tolist(),\n",
    "        \"stakeholders_importances\": stakeholder_importances.tolist(),\n",
    "        \"stakeholders_pbis_priorities\": stakeholder_pbis_priorities,\n",
    "        \"dependencies\": pbi_dependencies.tolist(),\n",
    "        \"_len_pbis_cost\": len(pbi_costs),\n",
    "        \"_len_stakeholders_importances\": len(stakeholder_importances),\n",
    "        \"_len_stakeholders_pbis_priorities\": len(stakeholder_pbis_priorities),\n",
    "        \"_len_dependencies\": len([x for x in pbi_dependencies if x is not None])\n",
    "    }\n",
    "\n",
    "    # store data in json file\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "    with open(output_file, \"w\", newline='\\n') as json_file:\n",
=======
    "    with open(output_file, \"w\") as json_file:\n",
>>>>>>> ecb85730 (now pbil and geneticnds keep nds from initial population, then pareto is now wider)
=======
    "    with open(output_file, \"w\", newline='\\n') as json_file:\n",
>>>>>>> d98580f6 (solved issue in mimic when sampling individuals)
    "        json.dump(json_data, json_file, indent=4)\n",
    "\n",
    "    # return dataset instance\n",
    "    dataset = Dataset(\"random\", source_file=output_file)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def _constrained_sum_sample_pos(num_pbis: int, total_sum_costs: int) -> np.ndarray:\n",
    "    \"\"\"Return a randomly chosen list of n positive integers summing to total.\n",
    "    Each such list is equally likely to occur.\"\"\"\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "    \n",
    "    dividers = sorted(random.sample(range(1, total_sum_costs), num_pbis - 1))\n",
    "    return np.array([a - b for a, b in zip(dividers + [total_sum_costs], [0] + dividers)])\n",
    "\n",
    "    \n",
    "    #costs = random.sample(range(1,int(total_sum_costs/num_pbis)), num_pbis)\n",
    "    #factor = total_sum_costs / sum(costs)\n",
    "    #costs = [cost * factor for cost in costs]\n",
    "    #return costs\n",
    "\n"
   ]
=======
    "\n",
    "    dividers = sorted(random.sample(range(1, total_sum_costs), num_pbis - 1))\n",
    "    return np.array([a - b for a, b in zip(dividers + [total_sum_costs], [0] + dividers)])\n"
<<<<<<< HEAD
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
>>>>>>> ecb85730 (now pbil and geneticnds keep nds from initial population, then pareto is now wider)
=======
=======
    "    \n",
    "    dividers = sorted(random.sample(range(1, total_sum_costs), num_pbis - 1))\n",
    "    return np.array([a - b for a, b in zip(dividers + [total_sum_costs], [0] + dividers)])\n",
    "\n",
    "    \n",
    "    #costs = random.sample(range(1,int(total_sum_costs/num_pbis)), num_pbis)\n",
    "    #factor = total_sum_costs / sum(costs)\n",
    "    #costs = [cost * factor for cost in costs]\n",
    "    #return costs\n",
    "\n"
>>>>>>> a7235ed3 (solved comments from pull request, added minor local changes in some files)
   ]
>>>>>>> d98580f6 (solved issue in mimic when sampling individuals)
  },
  {
   "cell_type": "markdown",
   "id": "continuing-english",
   "metadata": {},
   "source": [
    "## Classic project datasets\n",
    "More stakeholders and requirements to plan for a release, less interaction between them, generic requirement estimations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-scene",
   "metadata": {},
   "source": [
    "Total costs were extracted from ISBSG 2015, using {A,B} values for \"UFP rating\", \"New development\" for \"Development type\" and \"IFPUG 4+\" for \"Count approach\". This procedure is used to generate percentile 25,50,75 of total FPs of a classic project, in order to generate a realistic sample of classic estimation of requirements, done by selecting randomly, for a given number of pbis, a list of costs that sums up to the percentile value. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 37,
   "id": "6d5a12e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
=======
   "execution_count": 8,
>>>>>>> ecb85730 (now pbil and geneticnds keep nds from initial population, then pareto is now wider)
   "id": "diagnostic-behalf",
=======
   "execution_count": 37,
   "id": "6d5a12e8",
>>>>>>> a7235ed3 (solved comments from pull request, added minor local changes in some files)
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "reqs=[200, 300]\n",
=======
    "reqs=[50,100,200]\n",
>>>>>>> d98580f6 (solved issue in mimic when sampling individuals)
=======
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "diagnostic-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reqs=[200, 300]\n",
>>>>>>> a7235ed3 (solved comments from pull request, added minor local changes in some files)
    "stkh=[15,50]\n",
    "percentile_total_costs=[156,312,705]\n",
    "range_stakeholder_importances=[1,3,5]\n",
    "range_stakeholder_pbis_priorities=[1,3,5]\n",
    "\n",
    "percentage_dependencies_min= 0.4\n",
    "percentage_dependencies_max= 0.5\n",
    "len_dependencies = [2] # None\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "counter=0\n",
    "perc=2\n",
=======
    "counter=9\n",
    "perc=0\n",
>>>>>>> ecb85730 (now pbil and geneticnds keep nds from initial population, then pareto is now wider)
=======
    "counter=0\n",
    "perc=2\n",
>>>>>>> a7235ed3 (solved comments from pull request, added minor local changes in some files)
    "for r in reqs:\n",
    "    for s in stkh:\n",
    "        for len_deps in len_dependencies:\n",
    "            counter+=1\n",
    "            name=f\"e{counter}\"\n",
    "            percentage_dependencies = random.uniform(percentage_dependencies_min,percentage_dependencies_max)\n",
    "            random_dataset_generator(num_pbis = r, num_stakeholders=s, percentage_dependencies=percentage_dependencies,\n",
    "                                    total_pbi_costs=percentile_total_costs[2], #greatest percentile when there are more attributes\n",
    "                                    range_stakeholder_importances=range_stakeholder_importances,\n",
    "                                    range_stakeholder_pbis_priorities=range_stakeholder_pbis_priorities, name=name, avg_len_dependencies=len_deps)\n",
    "    perc+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-necklace",
   "metadata": {},
   "source": [
    "## Agile project datasets\n",
    "Less stakeholders and requirements to plan for a release, much more interaction between stakeholder interests, Fibonacci estimations."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 6,
>>>>>>> ecb85730 (now pbil and geneticnds keep nds from initial population, then pareto is now wider)
=======
   "execution_count": 4,
>>>>>>> d98580f6 (solved issue in mimic when sampling individuals)
   "id": "empty-special",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> d98580f6 (solved issue in mimic when sampling individuals)
      "d1\n",
      "d2\n",
      "d3\n",
      "d4\n",
      "d5\n",
      "d6\n"
<<<<<<< HEAD
=======
      "d1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\PABLO~1.BER\\AppData\\Local\\Temp/ipykernel_7332/3537749295.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m             \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34mf\"d{counter}\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m             random_dataset_generator(num_pbis = r, num_stakeholders=s, percentage_dependencies=percentage_dependencies,\n\u001B[0m\u001B[0;32m     17\u001B[0m                                  \u001B[0mrange_pbi_costs\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0mrange_pbi_costs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrange_stakeholder_importances\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrange_stakeholder_importances\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m                                  range_stakeholder_pbis_priorities=range_stakeholder_pbis_priorities, name=name, avg_len_dependencies=numDeps)\n",
      "\u001B[1;32mC:\\Users\\PABLO~1.BER\\AppData\\Local\\Temp/ipykernel_7332/3010358782.py\u001B[0m in \u001B[0;36mrandom_dataset_generator\u001B[1;34m(num_pbis, num_stakeholders, percentage_dependencies, range_pbi_costs, total_pbi_costs, range_stakeholder_importances, range_stakeholder_pbis_priorities, name, avg_len_dependencies)\u001B[0m\n\u001B[0;32m     50\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnum_stakeholders\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m         priorities = np.random.choice(\n\u001B[1;32m---> 52\u001B[1;33m             range_stakeholder_pbis_priorities, size=num_pbis)\n\u001B[0m\u001B[0;32m     53\u001B[0m         \u001B[0mstakeholder_pbis_priorities\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpriorities\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Users\\PABLO~1.BER\\AppData\\Local\\Temp/ipykernel_7332/3010358782.py\u001B[0m in \u001B[0;36mrandom_dataset_generator\u001B[1;34m(num_pbis, num_stakeholders, percentage_dependencies, range_pbi_costs, total_pbi_costs, range_stakeholder_importances, range_stakeholder_pbis_priorities, name, avg_len_dependencies)\u001B[0m\n\u001B[0;32m     50\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnum_stakeholders\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m         priorities = np.random.choice(\n\u001B[1;32m---> 52\u001B[1;33m             range_stakeholder_pbis_priorities, size=num_pbis)\n\u001B[0m\u001B[0;32m     53\u001B[0m         \u001B[0mstakeholder_pbis_priorities\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpriorities\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2021.3\\plugins\\python\\helpers\\pydev\\pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1145\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1146\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1147\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1148\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1149\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2021.3\\plugins\\python\\helpers\\pydev\\pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1160\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1161\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1162\u001B[1;33m                 \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.01\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1163\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1164\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
>>>>>>> ecb85730 (now pbil and geneticnds keep nds from initial population, then pareto is now wider)
=======
>>>>>>> d98580f6 (solved issue in mimic when sampling individuals)
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "reqs=[100, 200, 300]\n",
    "stkh=[15,50]\n",
    "len_dependencies = [2] # [None]\n",
=======
    "reqs=[100, 200]\n",
    "stkh=[15,50]\n",
    "len_dependencies = [3,4,5] # [None]\n",
>>>>>>> ecb85730 (now pbil and geneticnds keep nds from initial population, then pareto is now wider)
=======
    "reqs=[100, 200, 300]\n",
    "stkh=[15,50]\n",
    "len_dependencies = [2] # [None]\n",
>>>>>>> d98580f6 (solved issue in mimic when sampling individuals)
    "range_pbi_costs =[1,1,2,3,5,8,13,21,34]\n",
    "range_stakeholder_importances=[1, 2, 3, 4, 5]\n",
    "range_stakeholder_pbis_priorities=[1, 2, 3, 4, 5]\n",
    "percentage_dependencies_min= 0.4\n",
    "percentage_dependencies_max= 0.5\n",
    "\n",
    "counter=0\n",
    "for r in reqs:\n",
    "    for s in stkh:\n",
    "        for numDeps in len_dependencies:\n",
<<<<<<< HEAD
<<<<<<< HEAD
    "            percentage_dependencies = random.uniform(percentage_dependencies_min,percentage_dependencies_max)\n",
=======
>>>>>>> ecb85730 (now pbil and geneticnds keep nds from initial population, then pareto is now wider)
=======
    "            percentage_dependencies = random.uniform(percentage_dependencies_min,percentage_dependencies_max)\n",
>>>>>>> d98580f6 (solved issue in mimic when sampling individuals)
    "            counter+=1\n",
    "            name=f\"d{counter}\"\n",
    "            print(name)\n",
    "            random_dataset_generator(num_pbis = r, num_stakeholders=s, percentage_dependencies=percentage_dependencies,\n",
    "                                 range_pbi_costs =range_pbi_costs, range_stakeholder_importances=range_stakeholder_importances,\n",
    "                                 range_stakeholder_pbis_priorities=range_stakeholder_pbis_priorities, name=name, avg_len_dependencies=numDeps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "72c0601c4fb6bbf9d900adba2b82670383af68cbbd272be51758c4d21ecb6472"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
